{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aparnamol-KS/CodeCompanion-GroqAI/blob/main/GenAI_Bootcamp_Part_A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df9218ce",
      "metadata": {
        "id": "df9218ce"
      },
      "source": [
        "# GenAI Bootcamp - LLMs, LangChain, LlamaIndex\n",
        "Welcome to Day 1 of the GenAI Bootcamp! This notebook covers foundational concepts and hands-on exercises for building with LangChain and LlamaIndex.\n",
        "\n",
        "**Topics Covered:**\n",
        "- Introduction to LLMs\n",
        "- LangChain basics with OpenAI\n",
        "- LlamaIndex for document querying\n",
        "- Mini project: Resume Q&A bot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c811c428",
      "metadata": {
        "id": "c811c428"
      },
      "source": [
        "## Introduction to LLMs\n",
        "Large Language Models (LLMs) are deep learning models trained on massive corpora of text data.\n",
        "\n",
        "**Popular LLMs:** GPT-4 (OpenAI), LLaMA (Meta), Mistral, Claude (Anthropic)\n",
        "\n",
        "**Key Concepts:**\n",
        "- Tokenization\n",
        "- Attention mechanism\n",
        "- In-context learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71452b92",
      "metadata": {
        "id": "71452b92"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7244371",
      "metadata": {
        "id": "d7244371"
      },
      "outputs": [],
      "source": [
        "# Install necessary packages\n",
        "!pip install --quiet langchain llama-index openai tiktoken faiss-cpu PyPDF2\n",
        "!pip install -q langchain-community langchainhub\n",
        "\n",
        "\n",
        "# Set OpenAI API Key\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = API_KEY\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# Use gpt-3.5-turbo or gpt-4-turbo instead\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "response = llm.predict(\"Explain large language models in one sentence\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "HPTtR6TqetPe"
      },
      "id": "HPTtR6TqetPe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In the context of language models like GPT, temperature is a parameter that controls the randomness or creativity of the modelâ€™s responses.\n",
        "\n",
        "ðŸ”¥ **Understanding Temperature**\n",
        "Range: Typically from **0.0 to 1.0** (can go up to 2.0 in some APIs, but rare).\n",
        "\n",
        "\n",
        "**Low Temperature (e.g., 0.0 to 0.3):**\n",
        "\n",
        "The model is more deterministic.\n",
        "\n",
        "It chooses the most likely next word.\n",
        "\n",
        "Responses are more focused, predictable, and reliable.\n",
        "\n",
        "Ideal for: factual answers, summaries, code generation.\n",
        "\n",
        "**High Temperature (e.g., 0.7 to 1.0):**\n",
        "\n",
        "The model becomes more creative and diverse.\n",
        "\n",
        "It explores less likely word choices.\n",
        "\n",
        "Responses may be more engaging, but also less accurate.\n",
        "\n",
        "Ideal for: storytelling, brainstorming, creative writing.\n",
        "\n",
        "Example:\n",
        "\n",
        "Prompt: \"Write a tagline for a space travel company.\"\n",
        "\n",
        "temperature = 0.2 â†’\n",
        "\"Experience space like never before.\" (safe, generic)\n",
        "\n",
        "temperature = 0.9 â†’\n",
        "\"Leave gravity behind. Chase the stars with us.\" (more flair and creativity)"
      ],
      "metadata": {
        "id": "VqWdqVINf8AO"
      },
      "id": "VqWdqVINf8AO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **When to Use What?**"
      ],
      "metadata": {
        "id": "t3gnCr7Dgu74"
      },
      "id": "t3gnCr7Dgu74"
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Use Case            | Suggested Temperature |\n",
        "| ------------------- | --------------------- |\n",
        "| Factual QA          | 0.0 â€“ 0.3             |\n",
        "| Code generation     | 0.1 â€“ 0.3             |\n",
        "| Creative writing    | 0.7 â€“ 1.0             |\n",
        "| Brainstorming ideas | 0.6 â€“ 0.9             |\n",
        "| Business summaries  | 0.3 â€“ 0.5             |\n"
      ],
      "metadata": {
        "id": "yaMhAu5agpmq"
      },
      "id": "yaMhAu5agpmq"
    },
    {
      "cell_type": "markdown",
      "id": "4c7f5ee1",
      "metadata": {
        "id": "4c7f5ee1"
      },
      "source": [
        "## LangChain - Basic Chatbot with OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13d75a03",
      "metadata": {
        "id": "13d75a03"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import HumanMessage\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
        "\n",
        "message = HumanMessage(content=\"What is LangChain and how is it used?\")\n",
        "messages = [message]\n",
        "response = llm(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import HumanMessage\n",
        "\n",
        "# Step 1: Define your template\n",
        "template = \"Explain the concept of {concept} in simple terms in a minimum of {min} and maximum of {max} lines.\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# Step 2: Format the prompt with a value\n",
        "messages = prompt.format_messages(concept=\"Neuron\", min=5, max=10)\n",
        "\n",
        "# Step 3: Run it through the model\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "response = llm(messages)\n",
        "\n",
        "# Step 4: Output the result\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "id": "ahxm4Ob2hn78"
      },
      "id": "ahxm4Ob2hn78",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"concept\"],\n",
        "    template=\"Explain the concept of {concept}\"\n",
        ")\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "print(chain.run({\"concept\": \"autoencoder\", \"language\": \"English\"}))\n"
      ],
      "metadata": {
        "id": "jxcCNFg0mY7_"
      },
      "id": "jxcCNFg0mY7_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define a second prompt\n",
        "\n",
        "second_prompt = PromptTemplate(\n",
        "    input_variables=[\"ml_concept\"],\n",
        "    template=\"Turn the concept description of {ml_concept} and describe it to me using emojis\",\n",
        ")\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
      ],
      "metadata": {
        "id": "DfTxbrrGnWwp"
      },
      "id": "DfTxbrrGnWwp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
        "\n",
        "# Run the chain specifying only the input variable for the first chain.\n",
        "explanation = overall_chain.run(\"cars\")\n",
        "print(explanation)"
      ],
      "metadata": {
        "id": "BsgonVrInflD"
      },
      "id": "BsgonVrInflD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload a PDF file (e.g., your resume)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "M6FyL2nN1xln"
      },
      "id": "M6FyL2nN1xln",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3543d917",
      "metadata": {
        "id": "3543d917"
      },
      "source": [
        "## LlamaIndex - Index and Query PDFs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "565fd39b",
      "metadata": {
        "id": "565fd39b"
      },
      "outputs": [],
      "source": [
        "# Install everything you need\n",
        "!pip install -q llama-index langchain openai faiss-cpu tiktoken PyPDF2 langchain-community langchainhub\n",
        "\n",
        "\n",
        "# LlamaIndex Components (0.10+)\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
        "from llama_index.readers.file import PDFReader\n",
        "\n",
        "# Read PDF\n",
        "reader = PDFReader()\n",
        "documents = reader.load_data(file='/content/CV_APARNAMOLKS.pdf')\n",
        "\n",
        "# Build index\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "# Query the index\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"Summarize my technical skills and education.\")\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3b8788f",
      "metadata": {
        "id": "a3b8788f"
      },
      "source": [
        "## Mini Project: Resume Q&A Bot\n",
        "**Goal:** Create a chatbot that answers questions based on your resume PDF.\n",
        "\n",
        "**Steps:**\n",
        "1. Upload your resume as `resume.pdf`\n",
        "2. Use LlamaIndex to index the document\n",
        "3. Query it using natural language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08cd131b",
      "metadata": {
        "id": "08cd131b"
      },
      "outputs": [],
      "source": [
        "!pip install -q llama-index langchain openai faiss-cpu tiktoken PyPDF2 langchain-community langchainhub\n",
        "\n",
        "from google.colab import files\n",
        "from llama_index.readers.file import PDFReader\n",
        "from llama_index.core import VectorStoreIndex\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Extract the uploaded file name\n",
        "filename = next(iter(uploaded))\n",
        "\n",
        "reader = PDFReader()\n",
        "documents = reader.load_data(file=filename)\n",
        "\n",
        "# Build vector index\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "# Query the document\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"Summarize my key skills and experience.\")\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate(input_variables=[\"topic\"], template=\"Explain {topic} in simple terms.\")\n",
        "llm = ChatOpenAI()\n",
        "chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "print(chain.run(\"quantum computing\"))\n"
      ],
      "metadata": {
        "id": "XcaagFon17nk"
      },
      "id": "XcaagFon17nk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "\n",
        "response = openai.images.generate(\n",
        "    model=\"dall-e-3\",\n",
        "    prompt=\"I need a sign language image of saying I love you\",\n",
        "    size=\"1024x1024\",\n",
        "    quality=\"standard\",  # or \"hd\" if you're using GPT-4 Pro\n",
        "    n=1\n",
        ")\n",
        "\n",
        "# Extract and print image URL\n",
        "image_url = response.data[0].url\n",
        "print(\"Generated image URL:\", image_url)\n"
      ],
      "metadata": {
        "id": "2ap09ZeK7djZ"
      },
      "id": "2ap09ZeK7djZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install necessary packages and ffmpeg\n",
        "!pip install --upgrade openai\n",
        "!apt-get -y install ffmpeg\n",
        "\n",
        "\n",
        "# Step 3: Imports\n",
        "import openai\n",
        "import requests\n",
        "import time\n",
        "from IPython.display import Video, display\n",
        "\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Step 4: Define prompts for each video frame\n",
        "scene_prompts = [\n",
        "    \"A sunrise over a quiet mountain village, soft golden light\",\n",
        "    \"A child flying a kite on a green hill under a bright blue sky\",\n",
        "    \"A spaceship launching from a futuristic city at sunset\",\n",
        "    \"A thunderstorm over the ocean with crashing waves\",\n",
        "    \"A calm starry night in the desert with glowing cacti\"\n",
        "]\n",
        "\n",
        "image_paths = []\n",
        "\n",
        "# Step 5: Generate images using DALLÂ·E 3 and save locally\n",
        "for i, prompt in enumerate(scene_prompts):\n",
        "    print(f\"Generating frame {i+1} for prompt: {prompt}\")\n",
        "    response = openai.images.generate(\n",
        "        model=\"dall-e-3\",\n",
        "        prompt=prompt,\n",
        "        size=\"1024x1024\",\n",
        "        n=1\n",
        "    )\n",
        "    image_url = response.data[0].url\n",
        "\n",
        "    # Download the image\n",
        "    img_data = requests.get(image_url).content\n",
        "    file_name = f\"frame_{i:03d}.png\"\n",
        "    with open(file_name, \"wb\") as f:\n",
        "        f.write(img_data)\n",
        "    image_paths.append(file_name)\n",
        "\n",
        "    # Wait a bit to avoid rate limits\n",
        "    time.sleep(2)\n",
        "\n",
        "print(\"All frames generated and saved.\")\n",
        "\n",
        "# Step 6: Create video from generated images using ffmpeg\n",
        "print(\"Creating video from frames...\")\n",
        "!ffmpeg -framerate 1 -i frame_%03d.png -c:v libx264 -r 30 -pix_fmt yuv420p dalle_video.mp4\n",
        "\n",
        "print(\"Video creation complete!\")\n",
        "\n",
        "# Step 7: Display the generated video\n",
        "display(Video(\"dalle_video.mp4\", embed=True))\n"
      ],
      "metadata": {
        "id": "zIY5l-rRjJfd"
      },
      "id": "zIY5l-rRjJfd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MINI-PROJECT - Resume Q&A bot\n"
      ],
      "metadata": {
        "id": "DjgNGw5tJtd9"
      },
      "id": "DjgNGw5tJtd9"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio groq faiss-cpu sentence-transformers pyPDF2 gradio numpy"
      ],
      "metadata": {
        "id": "4wciJmPhlY37"
      },
      "id": "4wciJmPhlY37",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "import faiss\n",
        "import numpy as np\n",
        "from PyPDF2 import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from groq import Groq\n",
        "\n",
        "# 2. Groq API Setup (use environment variable in production)\n",
        "# GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
        "client = Groq(api_key=\"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\")\n",
        "\n",
        "# 3. Helper functions\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    reader = PdfReader(pdf_file)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        if page.extract_text():\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "def split_into_chunks(text, chunk_size=500):\n",
        "    words = text.split()\n",
        "    return [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Global variables to hold index and chunks\n",
        "faiss_index = None\n",
        "text_chunks = []\n",
        "\n",
        "def process_pdf(pdf_file):\n",
        "    global faiss_index, text_chunks\n",
        "    raw_text = extract_text_from_pdf(pdf_file.name)\n",
        "    text_chunks = split_into_chunks(raw_text)\n",
        "    embeddings = embedding_model.encode(text_chunks)\n",
        "    dimension = embeddings.shape[1]\n",
        "    faiss_index = faiss.IndexFlatL2(dimension)\n",
        "    faiss_index.add(np.array(embeddings))\n",
        "    return \"PDF processed and vector index created successfully.\"\n",
        "\n",
        "def query_document(query_text, top_k=3):\n",
        "    if faiss_index is None or not text_chunks:\n",
        "        return \"Please upload and process a PDF first.\"\n",
        "\n",
        "    query_vector = embedding_model.encode([query_text])\n",
        "    distances, indices = faiss_index.search(np.array(query_vector), top_k)\n",
        "    context = \"\\n\\n\".join([text_chunks[i] for i in indices[0]])\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"llama3-8b-8192\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an assistant that summarizes and analyzes documents.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"{context}\\n\\nQuestion: {query_text}\"}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# 4. Gradio UI\n",
        "iface = gr.Interface(\n",
        "    fn=lambda file, question: (\n",
        "        process_pdf(file),         # status output\n",
        "        query_document(question)   # answer output\n",
        "    ),\n",
        "    inputs=[\n",
        "        gr.File(label=\"Upload your PDF\"),\n",
        "        gr.Textbox(label=\"Enter your question\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Status\", interactive=False),\n",
        "        gr.Textbox(label=\"Answer\", interactive=False)\n",
        "    ],\n",
        "    title=\"Resume Q&A Bot\",\n",
        "    description=\"Upload a PDF and ask questions about its content.\"\n",
        ")\n",
        "\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "id": "hz9SgmJZ3UJs"
      },
      "id": "hz9SgmJZ3UJs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RqTSXjiWIWyc"
      },
      "id": "RqTSXjiWIWyc",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}