{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aparnamol-KS/CodeCompanion-GroqAI/blob/main/Part_C_RAG_Bootcamp_GROQ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a17b03f",
      "metadata": {
        "id": "2a17b03f"
      },
      "source": [
        "# Day 3: Retrieval-Augmented Generation (RAG) with GROQ LLaMA3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bf63ca0",
      "metadata": {
        "id": "0bf63ca0"
      },
      "source": [
        "\n",
        "This notebook demonstrates how to build a Retrieval-Augmented Generation (RAG) system using **LangChain** and the **GROQ API** (LLaMA3 models).  \n",
        "We'll walk through creating embeddings, storing in a vector DB, and querying using a custom LangChain-compatible `GroqLLM`.\n",
        "\n",
        "## RAG = Retriever + Reader (LLM)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96c1bbec",
      "metadata": {
        "id": "96c1bbec"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install -U langchain langchain-community openai chromadb faiss-cpu sentence-transformers tiktoken groq\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "059a6698",
      "metadata": {
        "id": "059a6698"
      },
      "source": [
        "\n",
        "## Architecture Overview\n",
        "```text\n",
        "[User Query] → [Retriever → VectorDB] → [Relevant Chunks] → [GROQ LLM] → [Answer]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "649fbbbf",
      "metadata": {
        "id": "649fbbbf"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms.base import LLM\n",
        "\n",
        "from typing import List, Optional\n",
        "from groq import Groq\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50394900",
      "metadata": {
        "id": "50394900"
      },
      "source": [
        "## Step 1: Load Sample README File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ab4292f",
      "metadata": {
        "id": "9ab4292f"
      },
      "outputs": [],
      "source": [
        "\n",
        "sample_text = '''# Sample Project\n",
        "\n",
        "This project demonstrates an example of a LangChain-powered RAG pipeline. It uses FAISS for vector search and a GROQ-hosted LLaMA3 model for response generation.\n",
        "\n",
        "## Features\n",
        "\n",
        "- Document embedding\n",
        "- Vector similarity search\n",
        "- LLM-based QA over documents\n",
        "'''\n",
        "\n",
        "with open(\"sample_readme.txt\", \"w\") as f:\n",
        "    f.write(sample_text)\n",
        "\n",
        "loader = TextLoader(\"sample_readme.txt\")\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "docs = text_splitter.split_documents(documents)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed0f8a47",
      "metadata": {
        "id": "ed0f8a47"
      },
      "source": [
        "## Step 2: Create Embeddings & Store in Chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be83bfcd",
      "metadata": {
        "id": "be83bfcd"
      },
      "outputs": [],
      "source": [
        "\n",
        "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "vectorstore = Chroma.from_documents(docs, embedding, persist_directory=\"rag_chroma_groq\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7255c45e",
      "metadata": {
        "id": "7255c45e"
      },
      "source": [
        "## Step 3: Define GROQ LLM Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8760bca",
      "metadata": {
        "id": "b8760bca"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GroqLLM(LLM):\n",
        "    model: str = \"llama3-8b-8192\"\n",
        "    api_key: str = \"\"  # Replace with your actual API key\n",
        "    temperature: float = 0.0\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        client = Groq(api_key=self.api_key)\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=messages,\n",
        "            temperature=self.temperature,\n",
        "        )\n",
        "\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"groq-llm\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "601cd4b3",
      "metadata": {
        "id": "601cd4b3"
      },
      "source": [
        "## Step 4: Build RAG Pipeline with GROQ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fbaca0d",
      "metadata": {
        "id": "7fbaca0d"
      },
      "outputs": [],
      "source": [
        "\n",
        "retriever = vectorstore.as_retriever()\n",
        "groq_llm = GroqLLM(api_key=\"\")\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=groq_llm,\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "205bf587",
      "metadata": {
        "id": "205bf587"
      },
      "source": [
        "## Step 5: Ask a Question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21150843",
      "metadata": {
        "id": "21150843"
      },
      "outputs": [],
      "source": [
        "\n",
        "query = \"What does this project demonstrate?\"\n",
        "result = qa_chain({\"query\": query})\n",
        "print(\"Answer:\", result[\"result\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "StQJ84-SE22m"
      },
      "id": "StQJ84-SE22m",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}